<!DOCTYPE html>
<html>
<section class="section"><!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TerraMind: Large-Scale Generative Multimodality for Earth Observation">
  <meta name="keywords" content="TerraMind, geospatial AI, multi-modality, foundation models">
  <meta name="author" content="IBM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>IBM-ESA TerraMind</title>

  <!-- Fonts & CSS -->
  <link rel="shortcut icon" href="./favicon.ico?" type="image/x-icon">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- Light-gray background for every 2nd block -->
  <style>
    section.alt-bg:nth-of-type(odd){background:#f5f5f5;}
  </style>
</head>
<body>

<section class="hero is-medium is-parallax full-bleed hero-bg">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
<!--      <h1 class="title is-1 white">TerraMind Blue-Sky Challenge</h1>-->
      <h1 class="title is-1 white">
        <img src="./static/images/terramind.png" alt="Logo" class="full-image">
        TerraMind: Large-Scale Generative Multimodality for Earth Observation
      </h1>
      <br>
      <strong>Meet TerraMind</strong>, the first any-to-any generative, multimodal foundation model for Earth observation. TerraMind represents new levels of understanding geospatial data, introduces new capabilities such as Thinking-in-Modalities (TiM), and outperforms existing models significantly across community-standard benchmarks.
      <br>
      <br>
      <!-- submit button -->
      <a href="https://huggingface.co/ibm-esa-geospatial" class="button is-link is-normal is-rounded is-dark">
        Try it out on HuggingFace
      </a>
      <br>
      <br>
      <a href="https://arxiv.org/pdf/2504.11171" class="button is-link is-normal is-rounded is-dark">
        Find our arXiv preprint
      </a>
      <br>
      <br>
      <a href="https://arxiv.org/pdf/2504.11172" class="button is-link is-normal is-rounded is-dark">
        Learn more about the TerraMesh dataset
      </a>
      <br>
      <br>
      <a href="https://github.com/ibm/terramind" class="button is-link is-normal is-rounded is-dark">
        Find code for finetuning on GitHub
      </a>
      <br>
      <br>
      <a href="https://huggingface.co/spaces/ibm-esa-geospatial/challenge" class="button is-link is-normal is-rounded is-dark">
          TerraMind Blue-Sky Challenge
      </a>
    </div>
  </div>
</section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Architecture ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="section alt-bg full-bleed">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üí° How does TerraMind work?</h2>
    <ul class="content">
      <br>
      <p>TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes <strong>high-level contextual information to learn cross-modal relationships</strong>, while on a pixel level, TerraMind leverages <strong>fine-grained representations to capture critical spatial nuances</strong>.</p>
      <br>
      <img src="./static/images/terramind_architecture.png" alt="Architecture" class="full-image">
      <br>
      <br>
        TerraMind leverages autoencoder-based architectures with a <strong>quantization step in the bottleneck</strong> for imagelike modalities such as Sentinel-1, Sentinel-2, LULC, NDVI, and DEM. Tokenizer encoders process an input image and generate a latent representation for each 16√ó16 patch, which is then <strong>discretized with finite-scalar-quantization (FSQ)</strong> into one of N codewords.
      <br>
      <br>
      <img src="./static/images/tokenizers.png" alt="Tokenizer Architecture" class="two-thirds-image">
    </ul>
  </div>
</section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Benchmark performance ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="section alt-bg full-bleed">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üöÄ How does TerraMind compare to other models?</h2>
    <img src="./static/images/performance.png" alt="Radar Plot" class="two-thirds-image">
    <br>
    <br>
    TerraMind was <strong>benchmarked by ESA</strong> in both unimodal and multimodal settings following the community-standard PANGAEA benchmark. Overall, TerraMindv1-B <strong>outperforms all other GeoFMs by at least 3pp avg. mIoU</strong>. Importantly, TerraMind is the <strong>only foundation model approach in EO that outperforms task-specific U-Net models across the PANGAEA benchmark</strong>. Performance is approximately 2pp avg. mIoU higher for Terramindv1-L, with a peak of 5pp in multimodal datasets.
    <br>
    <br>
    <img src="./static/images/full_comparison.png" alt="Performance Table" class="full-image">
    "To me, what sets TerraMind apart is its <strong>ability to go beyond simply processing earth observations with computer vision algorithms</strong>. It instead has an <strong>intuitive understanding of geospatial data and our planet</strong>" said Juan Bernab√©-Moreno, director of IBM Research UK and Ireland, and IBM's Accelerated Discovery lead for climate and sustainability. "At present, TerraMind is the <strong>best performing AI foundation model for Earth observation</strong> according to well-established community-benchmarks" Bernab√©-Moreno added.
    <br>
    <br>
    "TerraMind combines insights from several modalities of training data to increase the accuracy of its outputs" said Simonetta Cheli, director of ESA Earth Observation Programmes and Head of ESRIN. "The ability to intuitively bring in contextual information and generate unseen scenarios is a <strong>critical step in unlocking the value of ESA data</strong>. Compared to competitive models, it <strong>can uncover a deeper understanding of the Earth for researchers and businesses alike</strong>."
  </div>
</section>


<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TiM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="section alt-bg full-bleed">
  <div class="container is-max-desktop">
    <h2 class="title is-3 mt-6">üí≠ What is Thinking-in-Modalities?</h2>
    During fine-tuning or inference, TerraMind can pause for a moment, imagine a helpful but absent layer, append the imagined tokens to its own input sequence, and then lets the fine-tuned encoder continue to improve its own performance. Because the imagination lives in token space, the approach <strong>avoids the heavy diffusion decoding that full image synthesis would require</strong>. So, TerraMind can generate any missing modality as an intermediate step ‚Äî an ability we call <strong>Thinking in Modalities (TiM)</strong>.
    <br>
    <img src="./static/images/tim.png" alt="TiM" class="two-thirds-image">
    <br>
    <br>
    "TiM tuning boosts data efficiency by <strong>self-generating the additional training data relevant to the problem being addressed</strong> ‚Äî for example, by telling the model to "think" about land cover when mapping water bodies. This breakthrough can unlock unprecedented accuracy when specializing TerraMind for particular use cases" said Johannes Jakubik, an IBM Research scientist based in Zurich.
    <br>
    <br>
    <img src="./static/images/tim_results.png" alt="TiM Results" class="two-thirds-image">
  </div>
</section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Few-Shot performance ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<section class="section alt-bg full-bleed">
  <div class="container is-max-desktop">
    <h2 class="title is-3">‚≠êÔ∏è Exploring the embedding space via one-shot classification</h2>
    <img src="./static/images/few_shot.png" alt="Few Shot" class="full-image">
    <br>
    TerraMind is pretrained on a <strong>cross-modal patch classification objective</strong>. Empirical results suggest that this results in a well-structured latent space that clusters different concepts accurately. To investigate this hypothesis, we apply 1-Nearest-Neighbor (1-NN) classification without applying any kind of weight updates. <strong>TerraMind outperforms other models significantly, pointing to a better structured embedding space</strong>.
  </div>
</section>

<!--&lt;!&ndash; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Benchmark performance ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ &ndash;&gt;-->
<!--<section class="section alt-bg full-bleed">-->
<!--  <div class="container is-max-desktop">-->
<!--    <h2 class="title is-3">üèÜ Need a challenge?</h2>-->
<!--    <br>-->
<!--    <div class="button-container">-->
<!--      <a href="https://huggingface.co/spaces/ibm-esa-geospatial/challenge" class="button is-link is-normal is-rounded is-dark">-->
<!--          TerraMind Blue-Sky Challenge-->
<!--      </a>-->
<!--    </div>-->
<!--    <br>-->
<!--    Find out more about our TerraMind Blue-Sky challenge here and the associated 1'000 EUR cash prizes.-->
<!--  </div>-->
<!--</section>-->

<section class="section alt-bg full-bleed">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üìΩÔ∏è Voices on TerraMind</h2>
    <br>
    <div class="video-container">
      <iframe width="560" height="315" src="https://dlmultimedia.esa.int/download/public/videos/2025/04/031/2504_031_AR_EN.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div>
    <br>
    <br>
<!--    ESA: "TerraMind is a powerful new AI tool designed to help us better understand and protect our planet. By combining different types of Earth observation data, it can deliver accurate answers to questions about climate and nature. From spotting methane leaks to tracking changes in forests and land use, TerraMind is set to play a key role in tackling some of the current environmental challenges."-->
<!--    <br>-->
    "With Earth observation science, technology, and international collaboration, we are unlocking the full potential of space-based data to protect our planet" said Nicolas Longepe, Earth Observation Data Scientist at ESA. "<strong>This project is a perfect example where the scientific community, big tech companies, and experts have collaborated to leverage this technology for the benefit of Earth sciences</strong>. The magic happens when earth observation data experts, machine learning experts, data scientists, and HPC engineers come together."
  </div>
</section>

<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FOOTER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
<footer class="footer full-bleed">
  <div class="container content has-text-centered">
    <p>¬© 2025 IBM & ESA ‚Äì TerraMind</p>
    <p>
      This site reuses elements from the
      <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies template</a>
      (CC-BY-SA 4.0).
    </p>
  </div>
</footer>
<script>
document.addEventListener('DOMContentLoaded', () => {
  const hero = document.querySelector('.hero-bg');
  if (!hero) return;                     // safety

  window.addEventListener('scroll', () => {
    const y = window.pageYOffset * 0.4;  // 0.2-0.5 = slower/faster
    hero.style.backgroundPosition = `center ${y}px`;
  });
});
</script>
</body>
</html>
</section>
</html>
